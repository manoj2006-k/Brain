<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <pre>In machine learning
        Main article: Neural network (machine learning)
        
        Schematic of a simple feedforward artificial neural network
        In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines,[3] today they are almost always implemented in software.
        
        Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).[4] The "signal" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.[5]
        
        The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers.
        
        Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.
        
        History
        See also: Biological neural network § History, and History of artificial neural networks
        The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873[6] and William James in 1890.[7] Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.[8]
        
        Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943,[9] followed by the implementation of one in hardware by Frank Rosenblatt in 1957,[3] artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.
        
        See also
        Emergence
        Biological cybernetics
        Biologically-inspired computing
        References
         Shao, Feng; Shen, Zheng (January 9, 2022). "How can artificial neural networks approximate the brain?". Front. Psychol. 13: 970214. doi:10.3389/fpsyg.2022.970214. PMC 9868316. PMID 36698593.
         Levitan, Irwin; Kaczmarek, Leonard (August 19, 2015). "Intercellular communication". The Neuron: Cell and Molecular Biology (4th ed.). New York, NY: Oxford University Press. pp. 153–328. ISBN 978-0199773893.
         Rosenblatt, F. (1958). "The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain". Psychological Review. 65 (6): 386–408. CiteSeerX 10.1.1.588.3775. doi:10.1037/h0042519. PMID 13602029. S2CID 12781225.
         Bishop, Christopher M. (August 17, 2006). Pattern Recognition and Machine Learning. New York: Springer. ISBN 978-0-387-31073-2.
         Vapnik, Vladimir N.; Vapnik, Vladimir Naumovich (1998). The nature of statistical learning theory (Corrected 2nd print. ed.). New York Berlin Heidelberg: Springer. ISBN 978-0-387-94559-0.
         Bain (1873). Mind and Body: The Theories of Their Relation. New York: D. Appleton and Company.
         James (1890). The Principles of Psychology. New York: H. Holt and Company.
         Hebb, D.O. (1949). The Organization of Behavior. New York: Wiley & Sons.
         McCulloch, W; Pitts, W (1943). "A Logical Calculus of Ideas Immanent in Nervous Activity". Bulletin of Mathematical Biophysics. 5 (4): 115–133. doi:10.1007/BF02478259. Archived from the original on May 17, 2024. Retrieved February 17, 2024.</pre>
</body>
</html>